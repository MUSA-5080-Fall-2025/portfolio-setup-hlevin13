{
  "hash": "251c2c3201872645928a3e9704e7e683",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor: \"Hope Levin & Annalise Abraham\"\ndate: \"2025-11-30\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: show\n    code_download: true\n---\n\n\n\n# Setup\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(Metrics)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n# Get rid of scientific notation.\noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n## Set Census API Key\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_api_key(\"5a82e243438bea307ae1c04f150d539c4db5fa47\", overwrite = TRUE, install = TRUE)\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n# PART 1.1: Data Import & Preparation\n\n## Load Indego Trip Data (Q2 2024)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q2 2024 data\nindego <- read_csv(\"indego-trips-2024-q2.csv\")\n```\n:::\n\n\n## Create Time Bins\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\nWe choose to examine Quarter 2 (April - June) in 2024, as it reflects the start of nice biking weather for 2024. Additionally, it was the farthest from Quarter 1 in 2025, which we believed could be interesting to examine.\n\n------------------------------------------------------------------------\n\n# PART 1.2: Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q2 2024\",\n    subtitle = \"Spring Demand Patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego Bike Share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\n\nDaily trip volumes rose from early April to mid-June. Mid-June to July marked a slight downfall, which we believe to be a result of riders opting for other transit methods due to high temperatures. Regardless, the overall trend can be described as a steady increase. Compared to Quarter 1 in 2025, the number of daily trips is much higher. We believe this to be a product of people opting to bike due to warmer/less harsh climate patterns. These riders could be commuters, or those looking for a fun and inexpensive outdoor activity.\n\n## Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear Commute Patterns on Weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\nHourly weekday ridership patterns indicate a rush hour peak time-based pattern. From the graph, we observed that peak weekday ride times are around 7am and 5pm. Hourly weekend ridership patterns are much different, with a gradual rise til about mid-day, followed by a gradual decline.\n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 6,115 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 5,231 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,295 </td>\n   <td style=\"text-align:right;\"> 39.95028 </td>\n   <td style=\"text-align:right;\"> -75.16027 </td>\n   <td style=\"text-align:right;\"> 4,451 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,248 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 4,070 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 4,052 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 4,047 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 3,847 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 3,792 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,208 </td>\n   <td style=\"text-align:right;\"> 39.95048 </td>\n   <td style=\"text-align:right;\"> -75.19324 </td>\n   <td style=\"text-align:right;\"> 3,661 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,052 </td>\n   <td style=\"text-align:right;\"> 39.94732 </td>\n   <td style=\"text-align:right;\"> -75.15695 </td>\n   <td style=\"text-align:right;\"> 3,651 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,012 </td>\n   <td style=\"text-align:right;\"> 39.94218 </td>\n   <td style=\"text-align:right;\"> -75.17747 </td>\n   <td style=\"text-align:right;\"> 3,573 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 3,558 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,244 </td>\n   <td style=\"text-align:right;\"> 39.93865 </td>\n   <td style=\"text-align:right;\"> -75.16674 </td>\n   <td style=\"text-align:right;\"> 3,549 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 3,523 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,007 </td>\n   <td style=\"text-align:right;\"> 39.94517 </td>\n   <td style=\"text-align:right;\"> -75.15993 </td>\n   <td style=\"text-align:right;\"> 3,492 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 3,470 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,443 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 3,425 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 3,404 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe top station reported 6,115 trips. The next highest station reported 5,231 trips, a 884 decline. Finally, the 20th highest station reported 3,404 trips. 13/20 of the top stations fell within the 3,000 - 3,999 trip range.\n\n------------------------------------------------------------------------\n\n# Get Philadelphia Spatial Context\n\n## Load Philadelphia Census Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for Understanding Bike Share Demand Patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\n\nThis map reveals that most Indego stations are concentrated in census tracts with a high median income, like Center City and Old City. However, many stations are located in lower income tracts, such as the University City area.\n\n## Join Census Data to Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Stations in Non-Residential Census Tracts\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% dplyr::select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      dplyr::select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% dplyr::select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego Stations Shown (RED = No Census Data Match)\",\n    caption = \"Red Xs indicate stations that did not join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n\nThe majority of the Indego stations joined to their respective census tracts. However, some stations around University City and the Sports Complex area did not join. Because these are areas that could be categorized as institutional or recreational, we believe that demand cannot be based on demographics. To elaborate, few people live in the Sports Complex area- but many travel there.\n\n## Dealing with Missing Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      dplyr::select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n## Get Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-04-01\",\n  date_end = \"2024-06-30\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  dplyr::select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n```\n:::\n\n\n## Visualize Weather Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q2 2024\",\n    subtitle = \"Spring to Early Summer Transition\",\n    x = \"Date\",\n    y = \"Temperature (Â°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\n\nOverall, temperatures rose in steady ebbs and flows from April into July. By July, temperatures were consistently over 80F, a big jump from the sub-50F temperatures in early April.\n\n------------------------------------------------------------------------\n\n# Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 175936\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 235\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2184\n```\n\n\n:::\n:::\n\n\n## Create Complete Panel Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 513,240 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 175,936 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 337,304 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 513,240 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n```\n:::\n\n\n------------------------------------------------------------------------\n\n# Create Temporal Lag Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 632,150 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past Demand Predicts Duture Demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\n\nBased on this visualization, we determined that hours with activity are typically followed by another hour of activity. This is called short-term persistence, or smooth demand changes.\n\n------------------------------------------------------------------------\n\n# Temporal Train/Test Split\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 23)\n\ntest <- study_panel_complete %>%\n  filter(week >= 23)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 447,528 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 176,552 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 19814 to 19876 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 19877 to 19904 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Build Predictive Models\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7197 -0.6759 -0.2053  0.1850 20.6050 \n\nCoefficients:\n                   Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.623380   0.014322 -43.527 < 0.0000000000000002 ***\nas.factor(hour)1  -0.072867   0.011938  -6.104 0.000000001037041261 ***\nas.factor(hour)2  -0.100186   0.011862  -8.446 < 0.0000000000000002 ***\nas.factor(hour)3  -0.101366   0.011827  -8.571 < 0.0000000000000002 ***\nas.factor(hour)4  -0.098172   0.011994  -8.185 0.000000000000000272 ***\nas.factor(hour)5   0.031520   0.011964   2.635             0.008424 ** \nas.factor(hour)6   0.251304   0.011708  21.464 < 0.0000000000000002 ***\nas.factor(hour)7   0.539861   0.011720  46.065 < 0.0000000000000002 ***\nas.factor(hour)8   0.854340   0.011759  72.652 < 0.0000000000000002 ***\nas.factor(hour)9   0.645589   0.011734  55.020 < 0.0000000000000002 ***\nas.factor(hour)10  0.548414   0.011459  47.857 < 0.0000000000000002 ***\nas.factor(hour)11  0.579472   0.011564  50.111 < 0.0000000000000002 ***\nas.factor(hour)12  0.626987   0.011257  55.699 < 0.0000000000000002 ***\nas.factor(hour)13  0.623662   0.011744  53.104 < 0.0000000000000002 ***\nas.factor(hour)14  0.656044   0.011488  57.107 < 0.0000000000000002 ***\nas.factor(hour)15  0.760629   0.011788  64.526 < 0.0000000000000002 ***\nas.factor(hour)16  0.862528   0.011435  75.426 < 0.0000000000000002 ***\nas.factor(hour)17  1.157733   0.011763  98.420 < 0.0000000000000002 ***\nas.factor(hour)18  0.930235   0.011977  77.668 < 0.0000000000000002 ***\nas.factor(hour)19  0.681722   0.011792  57.813 < 0.0000000000000002 ***\nas.factor(hour)20  0.432304   0.011798  36.642 < 0.0000000000000002 ***\nas.factor(hour)21  0.269929   0.011696  23.078 < 0.0000000000000002 ***\nas.factor(hour)22  0.191038   0.011726  16.291 < 0.0000000000000002 ***\nas.factor(hour)23  0.069112   0.011871   5.822 0.000000005812931084 ***\ndotw_simple2       0.061945   0.006312   9.814 < 0.0000000000000002 ***\ndotw_simple3       0.021134   0.006089   3.471             0.000519 ***\ndotw_simple4       0.103407   0.006324  16.352 < 0.0000000000000002 ***\ndotw_simple5       0.057970   0.006276   9.237 < 0.0000000000000002 ***\ndotw_simple6       0.007771   0.006527   1.191             0.233767    \ndotw_simple7      -0.034128   0.006504  -5.247 0.000000154440328780 ***\nTemperature        0.012729   0.000171  74.419 < 0.0000000000000002 ***\nPrecipitation     -2.054152   0.062168 -33.042 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.117 on 447496 degrees of freedom\nMultiple R-squared:  0.1143,\tAdjusted R-squared:  0.1142 \nF-statistic:  1863 on 31 and 447496 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1097 -0.4314 -0.1254  0.1217 17.3211 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.2516703  0.0123654 -20.353 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0315610  0.0102751  -3.072             0.002129 ** \nas.factor(hour)2  -0.0255020  0.0102137  -2.497             0.012531 *  \nas.factor(hour)3  -0.0193838  0.0101872  -1.903             0.057071 .  \nas.factor(hour)4  -0.0118938  0.0103351  -1.151             0.249811    \nas.factor(hour)5   0.0879106  0.0103161   8.522 < 0.0000000000000002 ***\nas.factor(hour)6   0.2413446  0.0101023  23.890 < 0.0000000000000002 ***\nas.factor(hour)7   0.3912950  0.0101257  38.644 < 0.0000000000000002 ***\nas.factor(hour)8   0.5751810  0.0101693  56.561 < 0.0000000000000002 ***\nas.factor(hour)9   0.2586771  0.0101578  25.466 < 0.0000000000000002 ***\nas.factor(hour)10  0.2239928  0.0099018  22.621 < 0.0000000000000002 ***\nas.factor(hour)11  0.2750789  0.0099905  27.534 < 0.0000000000000002 ***\nas.factor(hour)12  0.3434636  0.0097188  35.340 < 0.0000000000000002 ***\nas.factor(hour)13  0.3325602  0.0101359  32.810 < 0.0000000000000002 ***\nas.factor(hour)14  0.3735677  0.0099128  37.686 < 0.0000000000000002 ***\nas.factor(hour)15  0.4382105  0.0101782  43.054 < 0.0000000000000002 ***\nas.factor(hour)16  0.5186079  0.0098810  52.485 < 0.0000000000000002 ***\nas.factor(hour)17  0.7372223  0.0101810  72.412 < 0.0000000000000002 ***\nas.factor(hour)18  0.3982131  0.0103986  38.295 < 0.0000000000000002 ***\nas.factor(hour)19  0.2607371  0.0102099  25.538 < 0.0000000000000002 ***\nas.factor(hour)20  0.1041311  0.0102076  10.201 < 0.0000000000000002 ***\nas.factor(hour)21  0.0773039  0.0100888   7.662  0.00000000000001829 ***\nas.factor(hour)22  0.0804148  0.0100993   7.962  0.00000000000000169 ***\nas.factor(hour)23  0.0141841  0.0102172   1.388             0.165062    \ndotw_simple2       0.0186587  0.0054346   3.433             0.000596 ***\ndotw_simple3      -0.0004020  0.0052441  -0.077             0.938896    \ndotw_simple4       0.0308340  0.0054469   5.661  0.00000001507660401 ***\ndotw_simple5       0.0029701  0.0054080   0.549             0.582861    \ndotw_simple6      -0.0249257  0.0056246  -4.432  0.00000935943656602 ***\ndotw_simple7      -0.0338659  0.0055995  -6.048  0.00000000146779797 ***\nTemperature        0.0037869  0.0001493  25.370 < 0.0000000000000002 ***\nPrecipitation     -0.9449920  0.0535786 -17.637 < 0.0000000000000002 ***\nlag1Hour           0.4030038  0.0013966 288.563 < 0.0000000000000002 ***\nlag3Hours          0.1234155  0.0013804  89.408 < 0.0000000000000002 ***\nlag1day            0.1253160  0.0012810  97.830 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.961 on 447493 degrees of freedom\nMultiple R-squared:  0.344,\tAdjusted R-squared:  0.344 \nF-statistic:  6903 on 34 and 447493 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0962 -0.6781 -0.2691  0.4234 17.2038 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.3857918409  0.0371091868  10.396\nas.factor(hour)1         -0.0021036158  0.0412694199  -0.051\nas.factor(hour)2         -0.0088620570  0.0469025109  -0.189\nas.factor(hour)3         -0.0851082042  0.0529075781  -1.609\nas.factor(hour)4         -0.1044632428  0.0557223076  -1.875\nas.factor(hour)5          0.0516286807  0.0371091993   1.391\nas.factor(hour)6          0.2505851189  0.0312856619   8.010\nas.factor(hour)7          0.4005664111  0.0294980575  13.579\nas.factor(hour)8          0.6170072423  0.0287282776  21.477\nas.factor(hour)9          0.1382271180  0.0288552539   4.790\nas.factor(hour)10         0.1084529302  0.0287107985   3.777\nas.factor(hour)11         0.1546773002  0.0286616605   5.397\nas.factor(hour)12         0.2287878417  0.0281439459   8.129\nas.factor(hour)13         0.2593480167  0.0286602368   9.049\nas.factor(hour)14         0.2789001366  0.0282806593   9.862\nas.factor(hour)15         0.3576516996  0.0284181466  12.585\nas.factor(hour)16         0.4807434184  0.0279992423  17.170\nas.factor(hour)17         0.8226919934  0.0281755098  29.199\nas.factor(hour)18         0.3568673063  0.0285563485  12.497\nas.factor(hour)19         0.1776173104  0.0286237376   6.205\nas.factor(hour)20         0.0116961313  0.0293501783   0.399\nas.factor(hour)21         0.0203613434  0.0299654314   0.679\nas.factor(hour)22         0.0304705944  0.0307099983   0.992\nas.factor(hour)23        -0.0160285584  0.0326523113  -0.491\ndotw_simple2              0.0547342235  0.0119297080   4.588\ndotw_simple3              0.0108137555  0.0116378166   0.929\ndotw_simple4              0.0381015979  0.0117220832   3.250\ndotw_simple5              0.0054589733  0.0118327085   0.461\ndotw_simple6              0.0459381855  0.0125566250   3.658\ndotw_simple7              0.0389827321  0.0126414408   3.084\nTemperature               0.0075088185  0.0003400333  22.083\nPrecipitation            -2.1950253458  0.1294673174 -16.954\nlag1Hour                  0.2941552792  0.0022571175 130.323\nlag3Hours                 0.0851609125  0.0023233110  36.655\nlag1day                   0.0958710202  0.0022118269  43.345\nMed_Inc.x                 0.0000001627  0.0000001133   1.435\nPercent_Taking_Transit.y -0.0032455105  0.0004140532  -7.838\nPercent_White.y           0.0032869902  0.0002079131  15.809\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                     0.959347    \nas.factor(hour)2                     0.850135    \nas.factor(hour)3                     0.107702    \nas.factor(hour)4                     0.060834 .  \nas.factor(hour)5                     0.164148    \nas.factor(hour)6         0.000000000000001159 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9         0.000001666444693830 ***\nas.factor(hour)10                    0.000159 ***\nas.factor(hour)11        0.000000067997969454 ***\nas.factor(hour)12        0.000000000000000435 ***\nas.factor(hour)13        < 0.0000000000000002 ***\nas.factor(hour)14        < 0.0000000000000002 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18        < 0.0000000000000002 ***\nas.factor(hour)19        0.000000000547584341 ***\nas.factor(hour)20                    0.690260    \nas.factor(hour)21                    0.496826    \nas.factor(hour)22                    0.321099    \nas.factor(hour)23                    0.623508    \ndotw_simple2             0.000004477560354546 ***\ndotw_simple3                         0.352792    \ndotw_simple4                         0.001153 ** \ndotw_simple5                         0.644551    \ndotw_simple6                         0.000254 ***\ndotw_simple7                         0.002045 ** \nTemperature              < 0.0000000000000002 ***\nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                            0.151217    \nPercent_Taking_Transit.y 0.000000000000004594 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.221 on 145195 degrees of freedom\n  (302295 observations deleted due to missingness)\nMultiple R-squared:  0.2319,\tAdjusted R-squared:  0.2317 \nF-statistic:  1185 on 37 and 145195 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.259619 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.2582656 \n```\n\n\n:::\n:::\n\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2635307 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.2621691 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# PART 1.3: Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.87 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.97 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\nModel 1 - Time + Weather This model can best be described as a basic baseline model that demonstrates how trip counts can be impacted by weather time. The R\\^2 was 0.11 meaning that Model 1 accounts for 11% of variations in trip counts. With an MAE score of .87, the model is not appropriate for predictions.\n\nModel 2 - Time + Weather + Temporal Lags This model added lagged trip counts, which could be used to determine if past demand predicts future demand. We found that use is correlated with time, with rush hours at 7am and 5pm as proof. The R\\^2 was 0.34 meaning Model 2 accounts for 34% of variations in trip counts. However, this lag allowed for better predictions, as the MAE fell to 0.71.\n\nModel 3 - Time + Weather + Temporal Lags + Demographics This model added socioeconomic data like median income to see how demographic data impacts bike use patterns. The R\\^2 was 0.23 meaning Model 3 accounts for 23% of variations in trip counts. MAE was 0.97 - aggressively high for making predictions.\n\nModel 4 - Time + Weather + Temporal Lags + Demographics + Station Fixed Effects This model added station fixed effects, which controlled for unobserved differences between stations. The R\\^2 was 0.26 meaning Model 4 accounts for 26% of variations in trip counts. The MAE was 0.95. Both R\\^2 and MAE were slight improvements from Model 3, but overall still a poor performing model.\n\nModel 5 - Time + Weather + Temporal Lags + Demographics + Station Fixed Effects + Rush Hour Interactions This model added rush hour interactions. From this, we learned that peaks differ on weekends and weekdays, and trip count varies based on month due to temperature/seasonal changes. The R\\^2 and MAE were the same as Model 4.\n\nConcluding, Model 2 is the best of the 5, but requires work to improve predictions.\n\nCompared to Quarter 1 2025, we found that demand is higher in Quarter 2 2024 due to the introduction of warmer weather. However, given transition between seasons, Quarter 2 suffers from highs and lows in use. Both Quarter 1 2025 and Quarter 2 2024 had similar MAE scores. Moving forward, we now know that ridership is highly dependent on temperature, which we cannot predict, especially due to irregular weather patterns from climate change.\n\n------------------------------------------------------------------------\n\n# PART 2.1: Space-Time Error Analysis\n\n## Observed vs. Predicted\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 Performance by Time Period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n## Spatial Error Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/spatial_errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\np1\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/spatial_errors-2.png){width=672}\n:::\n:::\n\n\nPrediction errors are highest in Center City, which we know to be the area with highest demand, an attribute that stems from it having the most stations and highest ridership (and perhaps the most protective bike lanes). Because Center City is home to many landmarks and office buildings, much of its data is impacted by non-residents/tourists and commuters. Other neighborhoods have a lower MAE because they have stable use, a factor we believe stems from majority use by residents. Continuing, a high MAE and high demand score indicate that the model's poor performance comes from high variability\n\n## Temporal Error Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\n\nRegardless of day type, the model struggles most during the PM Rush with an MAE score above 1 for both weekdays and weekends- awful predictive performance! Overnight weekday and weekend MAE scores were the lowest, with figures just above 0.25, suggesting strong predictive performance.\n\n## Errors and Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% dplyr::select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\nErrors arose when incorporating median income and percent of population white. For example, a median income of \\$150,000 meant an MAE score of \\~1.25 and 75% white population meant an MAE score of \\~1.15. When you went back to 0 for both, the MAE scores were \\~0.75. However, the model performed much better as percent of population taking transit rose. In short, the model performs poorly in richer and/or whiter areas, and performs better in transit heavy areas. Concluding, we now know the model does not suffer from under-performance due to systematic biases.\n\n------------------------------------------------------------------------\n\n# PART 3.1: Feature Engineering & Model Improvement\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Perfect Biking Weather\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    perfect_biking_weather = if_else(\n      Temperature >= 60 & Temperature <= 75 &\n        Precipitation == 0,\n      1L,  # Yes\n      0L   # No\n    )\n  )\n\n# 2. Distance to City Hall\n\n# Get unique station locations\nstations_sf <- study_panel_complete %>%\n  distinct(start_station, start_lon.y, start_lat.y) %>%\n  st_as_sf(coords = c(\"start_lon.y\", \"start_lat.y\"), crs = 4326) %>%   # WGS84\n  st_transform(2272)  # PA South (ft) â good for Philly distances\n\n# City Hall coordinates (approx)\ncity_hall <- st_sfc(\n  st_point(c(-75.1636, 39.9526)),  # (lon, lat)\n  crs = 4326\n) %>%\n  st_transform(2272)  # match CRS of stations\n\n#compute distance from each station to center city\nlibrary(units)\n\nstations_sf <- stations_sf %>%\n  mutate(\n    dist_centercity_ft = as.numeric(st_distance(geometry, city_hall)),\n    dist_centercity_mi = as.numeric((dist_centercity_ft)/5280)\n  )\n\nstudy_panel_complete <- study_panel_complete %>%\n  left_join(\n    stations_sf %>%\n      st_drop_geometry() %>%\n      select(start_station, dist_centercity_mi),\n    by = \"start_station\"\n  )\n\n# 3. Distance to universities\nuniversities_sf <- tribble(\n  ~univ_name, ~lon,       ~lat,\n  \"Penn\",     -75.1910,   39.9515,\n  \"Drexel\",   -75.1899,   39.9566,\n  \"Temple\",   -75.1554,   39.9812,\n  \"StJoes\",   -75.2072,   39.9467\n) %>%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %>%\n  st_transform(st_crs(stations_sf))  # match station CRS\n\n# Distance matrix: stations (rows) x universities (cols)\ndist_mat <- st_distance(stations_sf, universities_sf)  # units: same as CRS (likely feet)\n\nstations_sf <- stations_sf %>%\n  mutate(\n    dist_nearest_univ_ft = apply(dist_mat, 1, min) |> as.numeric(),\n    dist_nearest_univ_mi = dist_nearest_univ_ft / 5280  # if CRS is feet (e.g., EPSG:2272)\n  )\n\nstudy_panel_complete <- study_panel_complete %>%\n  left_join(\n    stations_sf %>%\n      st_drop_geometry() %>%\n      select(start_station, dist_nearest_univ_mi),\n    by = \"start_station\"\n  )\n\n# 4. University in session variable\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    # Define the window of interest\n    in_apr_jun_2024 = (date >= as.Date(\"2024-01-16\") & date <= as.Date(\"2024-05-10\")),\n\n    # Because AprilâMay 2024 covers Penn, Temple, Drexel spring terms; June covers Drexel spring / maybe summer\n    university_in_session = if_else(in_apr_jun_2024, 1L, 0L)\n  ) %>%\n  select(-in_apr_jun_2024)  # optional drop of helper var\n\n# 5. Seven Day Rolling Average\nlibrary(slider)\n\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    date = as.Date(date)  # ensure a daily ordering key\n  ) %>%\n  group_by(start_station) %>%\n  arrange(start_station, date) %>%\n  mutate(\n    rolling_7day_avg = slide_dbl(\n      Trip_Count,\n      mean,\n      .before = 6,           # previous 6 days + current day = 7-day avg\n      .complete = FALSE       # first few days allowed NA\n    )\n  ) %>%\n  ungroup()\n\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 23)\n\ntest <- study_panel_complete %>%\n  filter(week >= 23)\n\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n```\n:::\n\n\n## Added Feature #1: Perfect Biking Weather\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel6 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + perfect_biking_weather,\n  data = train\n)\n\ncat(\"Model 6 R-squared:\", summary(model6)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 6 R-squared: 0.3441047 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 6 Adj R-squared:\", summary(model6)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 6 Adj R-squared: 0.3440534 \n```\n\n\n:::\n:::\n\n\n## Added Feature #2: Rolling 7 Day Average\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel7 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + perfect_biking_weather + rolling_7day_avg,\n  data = train\n)\n\ncat(\"Model 7 R-squared:\", summary(model7)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 7 R-squared: 0.4710487 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 7 Adj R-squared:\", summary(model7)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 7 Adj R-squared: 0.4710062 \n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(broom)\n\ntidy(model7) %>%\n  filter(\n    !grepl(\"as.factor\\\\(hour\\\\)\", term),\n    !grepl(\"as.factor\\\\(start_station\\\\)\", term)\n  ) %>%\n  arrange(p.value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 14 Ã 5\n   term                    estimate std.error statistic   p.value\n   <chr>                      <dbl>     <dbl>     <dbl>     <dbl>\n 1 lag1Hour                0.0920    0.00157     58.5   0        \n 2 lag3Hours              -0.236     0.00166   -143.    0        \n 3 lag1day                 0.0673    0.00116     57.8   0        \n 4 rolling_7day_avg        1.03      0.00315    328.    0        \n 5 (Intercept)            -0.260     0.0113     -23.0   5.03e-117\n 6 Precipitation          -0.663     0.0483     -13.7   7.77e- 43\n 7 dotw_simple6           -0.0211    0.00506     -4.17  3.02e-  5\n 8 Temperature            -0.000508  0.000146    -3.48  4.96e-  4\n 9 perfect_biking_weather -0.00993   0.00305     -3.26  1.12e-  3\n10 dotw_simple7           -0.0161    0.00503     -3.19  1.40e-  3\n11 dotw_simple3            0.0142    0.00471      3.02  2.49e-  3\n12 dotw_simple2            0.0133    0.00488      2.72  6.45e-  3\n13 dotw_simple4            0.00941   0.00489      1.92  5.46e-  2\n14 dotw_simple5           -0.00483   0.00486     -0.994 3.20e-  1\n```\n\n\n:::\n:::\n\n\n## Poison Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel8 <- glm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + dist_nearest_univ_mi + rolling_7day_avg,\n  data = train\n)\n\nsummary(model8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + dist_nearest_univ_mi + \n    rolling_7day_avg, data = train)\n\nCoefficients:\n                       Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)          -0.2499873  0.0113252  -22.074  < 2e-16 ***\nas.factor(hour)1      0.0344083  0.0092292    3.728 0.000193 ***\nas.factor(hour)2      0.0955691  0.0091794   10.411  < 2e-16 ***\nas.factor(hour)3      0.1433758  0.0091616   15.650  < 2e-16 ***\nas.factor(hour)4      0.1780115  0.0092991   19.143  < 2e-16 ***\nas.factor(hour)5      0.2971176  0.0092859   31.997  < 2e-16 ***\nas.factor(hour)6      0.4587513  0.0090962   50.433  < 2e-16 ***\nas.factor(hour)7      0.6107886  0.0091176   66.990  < 2e-16 ***\nas.factor(hour)8      0.7685405  0.0091511   83.984  < 2e-16 ***\nas.factor(hour)9      0.4787327  0.0091464   52.341  < 2e-16 ***\nas.factor(hour)10     0.3811709  0.0089048   42.805  < 2e-16 ***\nas.factor(hour)11     0.3995715  0.0089796   44.498  < 2e-16 ***\nas.factor(hour)12     0.4069850  0.0087297   46.620  < 2e-16 ***\nas.factor(hour)13     0.3681722  0.0091027   40.446  < 2e-16 ***\nas.factor(hour)14     0.4095844  0.0089024   46.008  < 2e-16 ***\nas.factor(hour)15     0.4940573  0.0091417   54.044  < 2e-16 ***\nas.factor(hour)16     0.5730481  0.0088748   64.570  < 2e-16 ***\nas.factor(hour)17     0.7504108  0.0091427   82.077  < 2e-16 ***\nas.factor(hour)18     0.4536709  0.0093397   48.575  < 2e-16 ***\nas.factor(hour)19     0.2625912  0.0091686   28.640  < 2e-16 ***\nas.factor(hour)20     0.1218959  0.0091667   13.298  < 2e-16 ***\nas.factor(hour)21     0.0284425  0.0090610    3.139 0.001695 ** \nas.factor(hour)22     0.0032357  0.0090723    0.357 0.721351    \nas.factor(hour)23    -0.0554113  0.0091776   -6.038 1.56e-09 ***\ndotw_simple2          0.0136847  0.0048804    2.804 0.005047 ** \ndotw_simple3          0.0145342  0.0047094    3.086 0.002027 ** \ndotw_simple4          0.0099432  0.0048918    2.033 0.042091 *  \ndotw_simple5         -0.0044501  0.0048565   -0.916 0.359499    \ndotw_simple6         -0.0199184  0.0050510   -3.943 8.03e-05 ***\ndotw_simple7         -0.0153248  0.0050287   -3.047 0.002308 ** \nTemperature          -0.0006880  0.0001347   -5.106 3.29e-07 ***\nPrecipitation        -0.6496027  0.0481225  -13.499  < 2e-16 ***\nlag1Hour              0.0920745  0.0015725   58.552  < 2e-16 ***\nlag3Hours            -0.2362162  0.0016555 -142.690  < 2e-16 ***\nlag1day               0.0674049  0.0011641   57.905  < 2e-16 ***\ndist_nearest_univ_mi -0.0022673  0.0016369   -1.385 0.166033    \nrolling_7day_avg      1.0327944  0.0031530  327.558  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.7448006)\n\n    Null deviance: 630087  on 447527  degrees of freedom\nResidual deviance: 333292  on 447491  degrees of freedom\nAIC: 1138209\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n\n```{.r .cell-code}\noverdispersion_ratio <- sum(residuals(model8, type = \"pearson\")^2) / model8$df.residual\noverdispersion_ratio\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7448006\n```\n\n\n:::\n:::\n\n\n# PART 3.1: Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test),\n    pred6 = predict(model6, newdata = test),\n    pred7 = predict(model7, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\",\n    \"6. Model 2 + Perfect Biking Weather\",\n    \"7. Model 6 + Rolling 7 Day Average\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.87 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.97 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 6. Model 2 + Perfect Biking Weather </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 7. Model 6 + Rolling 7 Day Average </td>\n   <td style=\"text-align:right;\"> 0.65 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## **Implementation:**\n\nIn Model 6 we added a feature for 'Perfect Biking Weather,' which we defined as between 60 and 75 degrees with no precipitation. We chose to add this because we predicted that people would be more likely to bike when outside conditions were ideal. The MAE remained the same as it was for Model 2: .71.\n\nIn Model 7 we added a feature for a 'Rolling 7 Day Average,' which generates an average trip count over the past 7 days. We added this feature because we could see from the graphics we made that trip counts in general rise and fall gradually. The MAE dropped to .65, meaning that it is an improvement from Model 2.\n\n# PART 3.1b: Poisson Predictions and MAE\n\nUsing a Poisson model only improved the MAE by .0001, so we decided to disregard it and focus on Model 7.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest$pred_pois   <- predict(model8,   newdata = test, type=\"response\")\n\nvalid <- test %>%\n  filter(\n    !is.na(Trip_Count),\n    !is.na(pred_pois),\n    !is.na(pred7)\n  )\n\nModel_Comparison <- tibble(\n  Model = c(\"Linear Regression\", \"Poisson GLM\"),\n  RMSE  = c(\n    rmse(valid$Trip_Count, valid$pred7),\n    rmse(valid$Trip_Count, valid$pred_pois)\n  ),\n  MAE   = c(\n    mae(valid$Trip_Count, valid$pred7),\n    mae(valid$Trip_Count, valid$pred_pois)\n  )\n)\n\nModel_Comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã 3\n  Model              RMSE   MAE\n  <chr>             <dbl> <dbl>\n1 Linear Regression  1.01 0.652\n2 Poisson GLM        1.01 0.652\n```\n\n\n:::\n:::\n\n\n# PART 3.2: Space-Time Error Analysis\n\n## Observed vs. Predicted\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred7,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred7)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 7 Performance by Time Period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/obs_vs_pred_1-1.png){width=672}\n:::\n:::\n\n\n# PART 3.3: Spatial Error Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors7 <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends\n\n# Calculate station errors\nstation_errors7 <- test %>%\n  filter(!is.na(pred7)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np4 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors7,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 1,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np5 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors7,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 1,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p4, p5,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/spatial_errors_1-1.png){width=672}\n:::\n\n```{.r .cell-code}\np4\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/spatial_errors_1-2.png){width=672}\n:::\n:::\n\n\n# PART 3.4: Temporal Errors\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors7 <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors7, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#F54927\", \"Weekend\" = \"#F5B027\")) +\n  scale_y_continuous(\n    limits = c(0, 1.25),\n    breaks = c(0, .25, .50, .75, 1.0, 1.25)\n  ) +\n  labs(\n    title = \"Prediction Errors by Time Period: Model 7\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/temporal_errors_1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period: Model 2\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/temporal_errors_1-2.png){width=672}\n:::\n:::\n\n\n# PART 3.5: Errors and Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo7 <- station_errors7 %>%\n  left_join(\n    station_attributes %>% dplyr::select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np6 <- ggplot(station_errors_demo7, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np7 <- ggplot(station_errors_demo7, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np8 <- ggplot(station_errors_demo7, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p6, p7, p8, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Lab5_RMD_files/figure-html/errors_demographics_1-1.png){width=672}\n:::\n:::\n\n\nOur new final model (model 7) improved predictions temporally during the AM rush, Evening, Mid-Day, and PM rush.The PM rush improvement is significant because this is one area where we had noticed the model's poor performance earlier. Interestingly, model 7 had a slightly higher MAE for the Overnight period. The map of prediction errors shows that both models perform poorly around center city.\n\n------------------------------------------------------------------------\n\n# Part 4: Critical Reflection\n\nWe believe our final model is appropriate for Indego to use. Out of all the models we tested it performed the best. Prediction errors cause problems for rebalancing when demand is higher than expected and Indego is not able to readjust stations quickly enough. We recommend using this model if Indego is willing to accept this level of error - that will depend on their current models in use, and their staffing and time constraints. Further testing of the model is certainly necessary. Prediction errors are highest around center city, a very affluent area, which suggests that the risk of worsening existing disparities in bike access is not a major concern.\n\nThe model performs the worst closest to center city. It could likely be improved by adding a variable measuring each station's proximity to commercial businesses and restaurants. One limitation is that it has only been modeled and tested on one quarter of data. It should be tested on different time periods to determine if it is actually generally applicable.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}